\chapter{Theoretical background}

\section{Image Processing Fundamentals}

In digital signal processing, signals are defined as one-dimensional functions \(f(t)\). In digital image
processing, however, an image can be considered a ``two-dimensional function, \(f(x, y)\)``, with  the
parameters \(x\) and \(y\) denoting the planar coordinates of a given point, and the amplitude of the function
representing the intensity of the light in that particular point. The common nomenclature for the points
are ``picture elements, image elements, pels, and pixels``, with the pixel being most used in order
to refer to such elements. Due to having a physical representation, the pixels and, subsequently,
their coordinates, are finite. \cite{dipBook}

Each pixel value and position can be modified by different operators, which can be classified into ``linear``
or ``non-linear``. A given operator \(h\), that gives a given output when applied on a certain input \cite{dipBook}
\[h[f(x, y)] = g(x, y)\] 
is considered linear if it satisfies the following condition \cite{dipBook}
\[h[a f_1(x, y) + b f_2(x, y)] = a h[f_1(x, y)] + b h[f_2(x, y)] =
	a g_1(x, y) + b g_2(x, y)\]

Another classification criterion is the amount of data required to apply a certain operand, as well as the
resulting effect on the final image. Based on this, operand that only affect the intensity value of a pixel
can be considered ``single-pixel operations``, if they only account for the current pixel value when computing
a new value, and ``neighborhood operations``, when an area around the current point is relevand in determining
the output. If the operator instead affects the position of the current pixel in the resulting image, it is
considered a ``geometric transformation``. \cite{dipBook}

\subsection{Color Manipulation}

For gray-scale images, single-pixel operations, defined \cite{dipBook} by \[s = T(z)\], where \(z\)
represents the current value, \(T\) denotes the applied transformation and \(s\) represents the output
result, linearly change the intensity of the pixel. In most systems, the pixel values are encoded on 8 bits,
meaning that both the input and the output will be in the range 0-255.

However, for most practical purposes, color images are used. Those are composed of several separate functions,
one for each component of the desired ``color space``, commonly referred to as channels. In this case,
transformations can be applied to one or more channels, depending on the desired result. Such a transformation
is the conversion between the RGB color space to the YUV color space. For RGB encoding, each channel holds
an 8-bit value, denoting the quantity of red, green or blue that add up to a particular pixels color. The YUV
color space, however, is comprised of three channels of different sizes. The Y channel holds the pixel color
intensity, in the same manner as a 1-channel image would. The UV channels hold 4-bit values amounting to the
quantity of red and blue at a certain position. This is done in order to preserve image detail, whilst
sacrificing color data for more efficient storage. The conversion between the color spaces is done through
matrix multiplication \cite{rgb2yuv}
\[
	\begin{bmatrix}
		R \\
		G \\
		B \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		1 & 0        & 1.402    \\
		1 & -0.34414 & -0.71414 \\
		1 & 1.772    & 0        \\
	\end{bmatrix}
	\times
	\begin{bmatrix}
		Y       \\
		U - 128 \\
		V - 128 \\
	\end{bmatrix}
\]
and is used to change from multi-channel to single-channel, or vice-versa, depending on the circumstance.
\cite{rgb2yuv}

\subsection{2D Convolution}

The most common method for applying neighborhood transformations is through ``spatial filtering``, a term
borrowed from classical DSP. This is because a spatial filter ``modifies an image by replacing the value
of each pixel by a function of the values of the pixel and its neighbors``. This is accomplished by computing
the multiplication between a pixel neighborhood and a matrix of the same size as the area of interest. Such
matrices are formally known as ``kernels``, and their coefficients denote the type of filter being applied.
\cite{dipBook}

Because the kernel acts as a ``floating window``, it can easily be assimilated to the convolution operation
from 1-dimensional signal processing \cite{dipBook}
\[g(x) = \sum_{s=-a}^{a} w(s)f(x+s)\]
from which
the formula for the 2-dimensional convolution operation \cite{dipBook} can be extrapolated 
\[(w*f)(x, y) = \sum_{s=-a}^{a}	\sum_{t=-b}^{b} w(s, t)f(x + s, y + t)\] 
This, in turn, has to account for the kernel
traversing the entire image, meaning that the picture will need to be padded with additional data, in order to
ensure the edge pixels are covered by the convolution.

\subsection{Pixel Remapping}

Geometric transformations consist of changing the position of each pixels, usually to induce or correct pixel
placement errors. Such an operation can be denoted \cite{dipBook} by
\[
	\begin{bmatrix}
		x' \\
		y' \\
	\end{bmatrix}
	=
	\mathbf{T}
	\begin{bmatrix}
		x \\
		y \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		t_11 & t_12 \\
		t_21 & t_22 \\
	\end{bmatrix}
	\begin{bmatrix}
		x \\
		y \\
	\end{bmatrix}
\]
where \(x'\) and \(y'\) represent the new pixel coordinates. While this model can be used to compute the
operations of rotation, scaling and sheering, it cannot be used for translation. In order to accommodate
all ``affine transformations``, a 3x3 model \cite{dipBook} was proposed
\[
	\begin{bmatrix}
		x' \\
		y' \\
		1  \\
	\end{bmatrix}
	=
	\mathbf{A}
	\begin{bmatrix}
		x \\
		y \\
		1 \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		a_11 & a_12 & a_13 \\
		a_21 & a_22 & a_23 \\
		0    & 0    & 1    \\
	\end{bmatrix}
	\begin{bmatrix}
		x \\
		y \\
		1 \\
	\end{bmatrix}
\]

Each type of transformation is implemented and showcased in the application, with the help of the
\textbf{OpenCV} and \textbf{QtFramework} libraries.

\section{OpenCV Library}

The ``Open Source Computer Vision Library`` provides specific infrastructure for computer Vision and
machine learning, as well as general image processing algorithms and utilities. Boasting over ``2500
optimised algorithms`` and a community of ``more than 47 thousand people``, the library was chosen
due to its versatility, extensive documentation and abundance of community made resources. \cite{opencvAbout}

The main OpenCV functionalities that this project makes use of are the video capture module, used
to interface with the systems default camera, and the image processing module, used extensively in
order to exchange and modify image data throughout the entire application.

\subsection{Modules}

The \verb|VideoCapture| class ``provides C++ API for capturing video from cameras or for reading video
files and image sequences``. It allows for opening video streams based on camera index and desired
``Capture API backends``, as well as easy access to each recorded frame. On top of this, it provides
functionality for programmatically tweaking the parameters of the API and verification functions
for the proper opening and closing of the capture target, as well as the successful grab of each frame.
\cite{opencvVideoCapture}

In order to ensure the optimal user experience, all operations done one each frame have to fall within
the video captures sampling period. If this cannot be achieved, the framerate can be adjusted in order
to ensure minimal delays in grabbing and processing the following frame.

Provided in the module \verb|imgproc| are tools ``used to perform various linear or non-linear filtering
operations on 2D images`` stored as 2-dimensional \verb|cv::Mat| - data array ``compatible with the
majority of dense array types from the standard toolkits and SDKs``.  It represents one of the core
pillars of the project, as most operations provided are used in the generation of images with high
visual impact. \cite{opencvImproc}

\section{QT Framework}

Developed as ``cross-platform application development framework for desktop, embedded and mobile``, the
Qt project provides a means for designing and building graphical user interfaces using C++. It was chosen
due to its compatibility with OpenCV, as well as its similarity to other high-level frameworks for
developing cross-platform applications. The main component used in this project is the ``Widgets module``,
which allows for hierarchical GUI elements to be ``written directly in C++``.\cite{qtAbout}

\section{CMake}

Used for complex C++ projects, CMake ``manages the build process in an operating system and in a
compiler-independent manner``. Due to the external dependencies, as well as the project structure being
broken down into two distinct libraries, CMake was chosen because it was ``designed to support complex
directory hierarchies and applications dependent on several libraries``. Another significant factor in the
decision to use this particular build system was the innate compatibility with OpenCV as well as QT.
\cite{cmakeAbout}

